tm_map(mydata.corpus, removeWords, stopwords('english'))
# replace all the variations
for (c in seq(mydata.corpus))
{
for (l in 1:length(list$Language)) {
k <- strsplit(list$Variation[l], ",")
if (length(k[[1]] != 0)) {
for (v in 1:length(k[[1]])) {
mydata.corpus[[c]] <-
gsub(paste("\\b", k[[1]][v], "\\b", sep = ""),
list$lowerLanguage[l],
mydata.corpus[[c]])
}
}
}
}
# replace all the variations
for (c in seq(mydata.corpus))
{
for (s in 1:length(states$name)) {
mydata.corpus[[c]] <-
gsub(paste("\\b", states$capital[s], "\\b", sep = ""), states$name[s], mydata.corpus[[c]])
mydata.corpus[[c]] <-
gsub(paste("\\b", states$largest[s], "\\b", sep = ""), states$name[s], mydata.corpus[[c]])
}
}
# convert to Plain Text Document
mydata.corpus <- tm_map(mydata.corpus, PlainTextDocument)
for(i in 1:length(mydata.corpus)){
content <- mydata.corpus[[i]]$content
for(j in 1:length(list$lowerLanguage)){
found <- FALSE
lang <- list$lowerLanguage[j]
count.lang <- count(grep(paste("\\b", lang, "\\b", sep=""), content))
if(length(count.lang$x) > 0){
for(s in 1:50){
state <- states$name[s]
if(grepl(state,content)){
found <- TRUE
init.count <- loc.df[j,s]
loc.df[j,s] <- init.count + length(count.lang$x)
}
}
if(!found){
init.count <- loc.df[j,51]
loc.df[j,51] <- init.count + length(count.lang$x)
}
}
}
}
mydata.tdm <- TermDocumentMatrix(mydata.corpus)
tdm.onlytags <- mydata.tdm[rownames(mydata.tdm)%in%list$lowerLanguage,]
}
#
setwd("~/ParseHubData/Source_Data/usmap")
new_df <- cbind(state_df, t(loc.df[,1:50]))
new_df$total <- rowSums(new_df[,3:59])
write.csv(dataset, file = "industry.csv", row.names = FALSE)
write.csv(new_df, file = "industry.df.csv", row.names = FALSE)
gsub(paste("\\b", "angularjs", "\\b", sep = ""), "angular.js")
gsub(paste("\\b", "angularjs", "\\b", sep = ""), "angular", "angular.js")
gsub(paste("\\b", "angularjs", "\\b", sep = ""), "angular", "angular js")
gsub(paste("\\b", "angular.js", "\\b", sep = ""), "angular", "angular js")
library(ggplot2)
library(plyr)
library(wordcloud)
library(RColorBrewer)
library(stringr)
library(tm)
library(ggdendro)
library(dendextend)
library(ape)
library(rjson)
library(networkD3)
library(fpc)
library(cluster)
# define "tolower error handling" function
try.tolower = function(x)
{
# create missing value
y = NA
# tryCatch error
try_error = tryCatch(
tolower(x),
error = function(e)
e
)
# if not an error
if (!inherits(try_error, "error"))
y = tolower(x)
# result
return(y)
}
setwd("~/ParseHubData/Source_Data/tech_list")
list <- read.csv("lang_list_lf.csv", header = TRUE, stringsAsFactors = TRUE)
states <- read.csv("state_name.csv", header = TRUE, stringsAsFactors = TRUE)
loc.df <- data.frame(matrix(ncol = length(states$name) + 1, nrow = length(list$Language)))
colnames(loc.df)<- sapply(states$name, try.tolower)
rownames(loc.df)<- sapply(list$Language, try.tolower)
loc.df[is.na(loc.df)] <- 0
state_df <- data.frame(code = states$abbr, state = states$name)
states$name <- sapply(states$name, try.tolower)
states$abbr <- sapply(states$abbr, try.tolower)
states$capital <- sapply(states$capital, try.tolower)
states$largest <- sapply(states$largest, try.tolower)
setwd("~/ParseHubData/Source_Data/industry")
industry_list <-
c("financial", "tech", "healthcare", "telecom", "retail")
for (industry in industry_list) {
setwd(paste("~/ParseHubData/Source_Data/", industry, sep = ""))
file_list <- list.files(pattern = "results.csv")
for (file in file_list) {
# if the merged dataset doesn't exist, create it
if (!exists("dataset")) {
dataset <- read.csv(file, header = FALSE, stringsAsFactors = FALSE)
}
# if the merged dataset does exist, append to it
if (exists("dataset")) {
temp_dataset <-
read.csv(file, header = FALSE, stringsAsFactors = FALSE)
dataset <- rbind(dataset, temp_dataset)
rm(temp_dataset)
}
}
df <- data.frame(A = dataset$V1, B = dataset$V2)
df$D <- paste(df$A, df$B, sep = " ")
mydata.vectors <- df$D
#Change all variation to lower
list$Variation <- sapply(list$Variation, try.tolower)
list$lowerLanguage <- sapply(list$Language, try.tolower)
# build a corpus
mydata.corpus <- Corpus(VectorSource(mydata.vectors))
mydata.corpus <- tm_map(mydata.corpus,
content_transformer(function(x)
iconv(x, to = 'UTF-8-MAC', sub = 'byte')),
mc.cores = 1)
# remove unnecessary spaces
mydata.corpus <- tm_map(mydata.corpus, stripWhitespace)
# make each letter lowercase
mydata.corpus <-
tm_map(mydata.corpus, content_transformer(tolower), mc.cores = 1)
# replace ++ and #
replacePlus <-
content_transformer(function(x) {
return (gsub("++", "pp", x, fixed = TRUE))
})
replaceHash <-
content_transformer(function(x) {
return (gsub("#", "csharp", x, fixed = TRUE))
})
mydata.corpus <- tm_map(mydata.corpus, replacePlus, mc.cores = 1)
#mydata.corpus <- tm_map(mydata.corpus, replaceHash, mc.cores = 1)
# remove non-alphanumeric characters
removeNonAlphaNumeric <-
content_transformer(function(x) {
return (gsub("[^a-zA-Z2-4+#]", " ", x))
})
mydata.corpus <- tm_map(mydata.corpus, removeNonAlphaNumeric, mc.cores = 1)
# remove generic and custom stopwords
mydata.corpus <-
tm_map(mydata.corpus, removeWords, stopwords('english'))
# replace all the variations
for (c in seq(mydata.corpus))
{
for (l in 1:length(list$Language)) {
k <- strsplit(list$Variation[l], ",")
if (length(k[[1]] != 0)) {
for (v in 1:length(k[[1]])) {
mydata.corpus[[c]] <-
gsub(paste("\\b", k[[1]][v], "\\b", sep = ""),
list$lowerLanguage[l],
mydata.corpus[[c]])
}
}
}
}
# replace all the variations
for (c in seq(mydata.corpus))
{
for (s in 1:length(states$name)) {
mydata.corpus[[c]] <-
gsub(paste("\\b", states$capital[s], "\\b", sep = ""), states$name[s], mydata.corpus[[c]])
mydata.corpus[[c]] <-
gsub(paste("\\b", states$largest[s], "\\b", sep = ""), states$name[s], mydata.corpus[[c]])
}
}
# convert to Plain Text Document
mydata.corpus <- tm_map(mydata.corpus, PlainTextDocument)
for(i in 1:length(mydata.corpus)){
content <- mydata.corpus[[i]]$content
for(j in 1:length(list$lowerLanguage)){
found <- FALSE
lang <- list$lowerLanguage[j]
count.lang <- count(grep(paste("\\b", lang, "\\b", sep=""), content))
if(length(count.lang$x) > 0){
for(s in 1:50){
state <- states$name[s]
if(grepl(state,content)){
found <- TRUE
init.count <- loc.df[j,s]
loc.df[j,s] <- init.count + length(count.lang$x)
}
}
if(!found){
init.count <- loc.df[j,51]
loc.df[j,51] <- init.count + length(count.lang$x)
}
}
}
}
mydata.tdm <- TermDocumentMatrix(mydata.corpus)
tdm.onlytags <- mydata.tdm[rownames(mydata.tdm)%in%list$lowerLanguage,]
}
#
setwd("~/ParseHubData/Source_Data/usmap")
new_df <- cbind(state_df, t(loc.df[,1:50]))
new_df$total <- rowSums(new_df[,3:59])
write.csv(dataset, file = "industry.csv", row.names = FALSE)
write.csv(new_df, file = "industry.df.csv", row.names = FALSE)
library(ggplot2)
library(plyr)
library(wordcloud)
library(RColorBrewer)
library(stringr)
library(tm)
library(ggdendro)
library(dendextend)
library(ape)
library(rjson)
library(networkD3)
library(fpc)
library(cluster)
# define "tolower error handling" function
try.tolower = function(x)
{
# create missing value
y = NA
# tryCatch error
try_error = tryCatch(
tolower(x),
error = function(e)
e
)
# if not an error
if (!inherits(try_error, "error"))
y = tolower(x)
# result
return(y)
}
setwd("~/ParseHubData/Source_Data/tech_list")
list <- read.csv("lang_list_lf.csv", header = TRUE, stringsAsFactors = TRUE)
states <- read.csv("state_name.csv", header = TRUE, stringsAsFactors = TRUE)
loc.df <- data.frame(matrix(ncol = length(states$name) + 1, nrow = length(list$Language)))
colnames(loc.df)<- sapply(states$name, try.tolower)
rownames(loc.df)<- sapply(list$Language, try.tolower)
loc.df[is.na(loc.df)] <- 0
state_df <- data.frame(code = states$abbr, state = states$name)
states$name <- sapply(states$name, try.tolower)
states$abbr <- sapply(states$abbr, try.tolower)
states$capital <- sapply(states$capital, try.tolower)
states$largest <- sapply(states$largest, try.tolower)
setwd("~/ParseHubData/Source_Data/industry")
industry_list <-
c("financial", "tech", "healthcare", "telecom", "retail")
for (industry in industry_list) {
setwd(paste("~/ParseHubData/Source_Data/", industry, sep = ""))
file_list <- list.files(pattern = "results.csv")
for (file in file_list) {
# if the merged dataset doesn't exist, create it
if (!exists("dataset")) {
dataset <- read.csv(file, header = FALSE, stringsAsFactors = FALSE)
}
# if the merged dataset does exist, append to it
if (exists("dataset")) {
temp_dataset <-
read.csv(file, header = FALSE, stringsAsFactors = FALSE)
dataset <- rbind(dataset, temp_dataset)
rm(temp_dataset)
}
}
df <- data.frame(A = dataset$V1, B = dataset$V2)
df$D <- paste(df$A, df$B, sep = " ")
mydata.vectors <- df$D
#Change all variation to lower
list$Variation <- sapply(list$Variation, try.tolower)
list$lowerLanguage <- sapply(list$Language, try.tolower)
# build a corpus
mydata.corpus <- Corpus(VectorSource(mydata.vectors))
mydata.corpus <- tm_map(mydata.corpus,
content_transformer(function(x)
iconv(x, to = 'UTF-8-MAC', sub = 'byte')),
mc.cores = 1)
# remove unnecessary spaces
mydata.corpus <- tm_map(mydata.corpus, stripWhitespace)
# make each letter lowercase
mydata.corpus <-
tm_map(mydata.corpus, content_transformer(tolower), mc.cores = 1)
# replace ++ and #
replacePlus <-
content_transformer(function(x) {
return (gsub("++", "pp", x, fixed = TRUE))
})
replaceHash <-
content_transformer(function(x) {
return (gsub("#", "csharp", x, fixed = TRUE))
})
mydata.corpus <- tm_map(mydata.corpus, replacePlus, mc.cores = 1)
mydata.corpus <- tm_map(mydata.corpus, replaceHash, mc.cores = 1)
# remove non-alphanumeric characters
removeNonAlphaNumeric <-
content_transformer(function(x) {
return (gsub("[^a-zA-Z2-4+#]", " ", x))
})
mydata.corpus <- tm_map(mydata.corpus, removeNonAlphaNumeric, mc.cores = 1)
# remove generic and custom stopwords
mydata.corpus <-
tm_map(mydata.corpus, removeWords, stopwords('english'))
# replace all the variations
for (c in seq(mydata.corpus))
{
for (l in 1:length(list$Language)) {
k <- strsplit(list$Variation[l], ",")
if (length(k[[1]] != 0)) {
for (v in 1:length(k[[1]])) {
mydata.corpus[[c]] <-
gsub(paste("\\b", k[[1]][v], "\\b", sep = ""),
list$lowerLanguage[l],
mydata.corpus[[c]])
}
}
}
}
# replace all the variations
for (c in seq(mydata.corpus))
{
for (s in 1:length(states$name)) {
mydata.corpus[[c]] <-
gsub(paste("\\b", states$capital[s], "\\b", sep = ""), states$name[s], mydata.corpus[[c]])
mydata.corpus[[c]] <-
gsub(paste("\\b", states$largest[s], "\\b", sep = ""), states$name[s], mydata.corpus[[c]])
}
}
# convert to Plain Text Document
mydata.corpus <- tm_map(mydata.corpus, PlainTextDocument)
for(i in 1:length(mydata.corpus)){
content <- mydata.corpus[[i]]$content
for(j in 1:length(list$lowerLanguage)){
found <- FALSE
lang <- list$lowerLanguage[j]
count.lang <- count(grep(paste("\\b", lang, "\\b", sep=""), content))
if(length(count.lang$x) > 0){
for(s in 1:50){
state <- states$name[s]
if(grepl(state,content)){
found <- TRUE
init.count <- loc.df[j,s]
loc.df[j,s] <- init.count + length(count.lang$x)
}
}
if(!found){
init.count <- loc.df[j,51]
loc.df[j,51] <- init.count + length(count.lang$x)
}
}
}
}
mydata.tdm <- TermDocumentMatrix(mydata.corpus)
tdm.onlytags <- mydata.tdm[rownames(mydata.tdm)%in%list$lowerLanguage,]
}
#
setwd("~/ParseHubData/Source_Data/usmap")
new_df <- cbind(state_df, t(loc.df[,1:50]))
new_df$total <- rowSums(new_df[,3:59])
write.csv(dataset, file = "industry.csv", row.names = FALSE)
write.csv(new_df, file = "industry.df.csv", row.names = FALSE)
library(ggplot2)
library(plyr)
library(wordcloud)
library(RColorBrewer)
library(stringr)
library(tm)
library(ggdendro)
library(dendextend)
library(ape)
library(rjson)
library(networkD3)
library(fpc)
library(cluster)
setwd("~/ParseHubData/LRCode/tech_list")
list <-
read.csv("lang_list_lf.csv",
header = TRUE,
stringsAsFactors = TRUE)
industry_list <-
c("retail", "healthcare", "financial", "telecom", "tech")
# define "tolower error handling" function
try.tolower = function(x)
{
# create missing value
y = NA
# tryCatch error
try_error = tryCatch(
tolower(x),
error = function(e)
e
)
# if not an error
if (!inherits(try_error, "error"))
y = tolower(x)
# result
return(y)
}
for (industry in industry_list) {
setwd(paste("~/ParseHubData/LRCode/data/", industry, sep = ""))
file_list <- list.files(pattern = "results.csv")
for (file in file_list) {
# if the merged dataset doesn't exist, create it
if (!exists("dataset")) {
dataset <- read.csv(file, header = FALSE, stringsAsFactors = FALSE)
}
# if the merged dataset does exist, append to it
if (exists("dataset")) {
temp_dataset <-
read.csv(file, header = FALSE, stringsAsFactors = FALSE)
dataset <- rbind(dataset, temp_dataset)
rm(temp_dataset)
}
}
df <- data.frame(A = dataset$V1, B = dataset$V2)
df$D <- paste(df$A, df$B, sep = " ")
mydata.vectors <- df$D
#Change all variation to lower
list$variation <- sapply(list$variation, try.tolower)
list$lower.language <- sapply(list$language, try.tolower)
# build a corpus
mydata.corpus <- Corpus(VectorSource(mydata.vectors))
mydata.corpus <- tm_map(mydata.corpus,
content_transformer(function(x)
iconv(x, to = 'UTF-8-MAC', sub = 'byte')),
mc.cores = 1)
# remove unnecessary spaces
mydata.corpus <- tm_map(mydata.corpus, stripWhitespace)
# make each letter lowercase
mydata.corpus <-
tm_map(mydata.corpus, content_transformer(tolower), mc.cores = 1)
# replace ++ and #
replacePlus <-
content_transformer(function(x) {
return (gsub("++", "pp", x, fixed = TRUE))
})
replaceHash <-
content_transformer(function(x) {
return (gsub("#", "csharp", x, fixed = TRUE))
})
mydata.corpus <- tm_map(mydata.corpus, replacePlus, mc.cores = 1)
mydata.corpus <- tm_map(mydata.corpus, replaceHash, mc.cores = 1)
# remove non-alphanumeric characters
removeNonAlphaNumeric <-
content_transformer(function(x) {
return (gsub("[^a-zA-Z2-4+#]", " ", x))
})
mydata.corpus <-
tm_map(mydata.corpus, removeNonAlphaNumeric, mc.cores = 1)
# remove generic and custom stopwords
mydata.corpus <-
tm_map(mydata.corpus, removeWords, stopwords('english'))
# replace all the variations
for (c in seq(mydata.corpus))
{
for (l in 1:length(list$language)) {
k <- strsplit(list$variation[l], ",")
if (length(k[[1]] != 0)) {
for (v in 1:length(k[[1]])) {
mydata.corpus[[c]] <-
gsub(paste("\\b", k[[1]][v], "\\b", sep = ""),
list$lower.language[l],
mydata.corpus[[c]])
}
}
}
}
# convert to Plain Text Document
mydata.corpus <- tm_map(mydata.corpus, PlainTextDocument)
# build a term-document matrix
mydata.tdm <- TermDocumentMatrix(mydata.corpus)
#filter terms
tdm.onlytags <-
mydata.tdm[rownames(mydata.tdm) %in% list$lower.language,]
#top10
freq <- sort(rowSums(as.matrix(tdm.onlytags)), decreasing = TRUE)
sorted.df <- data.frame(lower.language = names(freq), freq = freq)
setwd("~/ParseHubData/LRCode/plots")
pdf(file = paste(industry, "_rankings.pdf", sep= ""))
# Bar plot
q <-
ggplot(data = sorted.df[sorted.df$freq > 0, ], aes(x = reorder(lower.language, freq), y =
freq)) +
geom_bar(stat = "identity",
width = 0.5,
fill = "steelblue") +
geom_text(aes(label=freq), size = 2, hjust= -0.5) +
ggtitle(toupper(industry)) +
xlab("") + ylab("Number of Job Postings") +
theme(text = element_text(size=10),
axis.text.x = element_text(angle = 45, hjust = 1)) +
coord_flip()
print(q)
list$industry <-
sorted.df[match(list$lower.language, sorted.df$lower.language), 2]
colnames(list)[colnames(list) == 'industry'] <- industry
dev.off()
}
