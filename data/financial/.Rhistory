# if the merged dataset doesn't exist, create it
if (!exists("dataset")) {
dataset <- read.csv(file, header = FALSE, stringsAsFactors = FALSE)
}
# if the merged dataset does exist, append to it
if (exists("dataset")) {
temp_dataset <-
read.csv(file, header = FALSE, stringsAsFactors = FALSE)
dataset <- rbind(dataset, temp_dataset)
rm(temp_dataset)
}
}
df <- data.frame(A = dataset$V1, B = dataset$V2)
df$D <- paste(df$A, df$B, sep = " ")
mydata.vectors <- df$D
# define "tolower error handling" function
try.tolower = function(x)
{
# create missing value
y = NA
# tryCatch error
try_error = tryCatch(
tolower(x),
error = function(e)
e
)
# if not an error
if (!inherits(try_error, "error"))
y = tolower(x)
# result
return(y)
}
#Change all variation to lower
list$Variation <- sapply(list$Variation, try.tolower)
list$lowerLanguage <- sapply(list$Language, try.tolower)
# build a corpus
mydata.corpus <- Corpus(VectorSource(mydata.vectors))
mydata.corpus <- tm_map(mydata.corpus,
content_transformer(function(x)
iconv(x, to = 'UTF-8-MAC', sub = 'byte')),
mc.cores = 1)
# remove unnecessary spaces
mydata.corpus <- tm_map(mydata.corpus, stripWhitespace)
# make each letter lowercase
mydata.corpus <-
tm_map(mydata.corpus, content_transformer(tolower), mc.cores = 1)
# replace ++ and #
replacePlus <-
content_transformer(function(x) {
return (gsub("++", "pp", x, fixed = TRUE))
})
replaceHash <-
content_transformer(function(x) {
return (gsub("#", "csharp", x, fixed = TRUE))
})
#mydata.corpus <- tm_map(mydata.corpus, replacePlus, mc.cores = 1)
#mydata.corpus <- tm_map(mydata.corpus, replaceHash, mc.cores = 1)
# remove non-alphanumeric characters
removeNonAlphaNumeric <-
content_transformer(function(x) {
return (gsub("[^a-z2-4+#]", " ", x))
})
mydata.corpus <-
tm_map(mydata.corpus, removeNonAlphaNumeric, mc.cores = 1)
# replace punctuation
replacePunctuation <-
function(x) {
return (gsub(".", " ", x))
}
#mydata.corpus <- tm_map(mydata.corpus, content_transformer(replacePunctuation), mc.cores = 1)
# remove generic and custom stopwords
mydata.corpus <-
tm_map(mydata.corpus, removeWords, stopwords('english'))
# replace all the variations
for (c in seq(mydata.corpus))
{
for (l in 1:length(list$Language)) {
k <- strsplit(list$Variation[l], ",")
if (length(k[[1]] != 0)) {
for (v in 1:length(k[[1]])) {
mydata.corpus[[c]] <-
gsub(paste("\\b", k[[1]][v], "\\b", sep = ""),
list$lowerLanguage[l],
mydata.corpus[[c]])
}
}
}
}
# convert to Plain Text Document
mydata.corpus <- tm_map(mydata.corpus, PlainTextDocument)
# build a term-document matrix
mydata.tdm <- TermDocumentMatrix(mydata.corpus)
tdm.onlytags <-
mydata.tdm[rownames(mydata.tdm) %in% list$lowerLanguage, ]
#top10
freq <- sort(rowSums(as.matrix(tdm.onlytags)), decreasing = TRUE)
wf <- data.frame(lowerLanguage = names(freq), freq = freq)
list$industry <- wf[match(list$lowerLanguage, wf$lowerLanguage), 2]
colnames(list)[colnames(list) == 'industry'] <- industry
}
View(wf)
View(list)
gsub("[^#]", "", "c#")
gsub("[^a-z#]", "", "c#")
gsub("[^a-z#]", "", "c#.net")
gsub("[^a-z#.]", "", "c#.net")
gsub("[^a-z#.]", "", "angular.js")
gsub("[^a-z#]", "", "angular.js")
gsub("[^a-z#]", "", "angular.js.")
gsub("[^a-z#]", "", "C#.net")
gsub("[^a-zA-Z#]", "", "C#.net")
gsub("[^a-zA-Z+#]", "", "c++")
gsub("[^a-zA-Z+#]", "", "d3.js")
gsub("[^a-zA-Z2-4+#]", "", "d3.js")
gsub("[^a-zA-Z2-4+#]", " ", "d3.js")
gsub("[^a-zA-Z#]", " ", "C#.net")
library(ggplot2)
library(plyr)
library(wordcloud)
library(RColorBrewer)
library(stringr)
library(tm)
library(ggdendro)
library(dendextend)
library(ape)
library(rjson)
library(networkD3)
library(fpc)
library(cluster)
setwd("~/ParseHubData/Source_Data/tech_list")
list <-
read.csv("lang_list_lf.csv",
header = TRUE,
stringsAsFactors = TRUE)
setwd("~/ParseHubData/Source_Data/industry")
industry_list <-
c("healthcare", "retail")
for (industry in industry_list) {
setwd(paste("~/ParseHubData/Source_Data/", industry, sep = ""))
file_list <- list.files(pattern = "results.csv")
for (file in file_list) {
# if the merged dataset doesn't exist, create it
if (!exists("dataset")) {
dataset <- read.csv(file, header = FALSE, stringsAsFactors = FALSE)
}
# if the merged dataset does exist, append to it
if (exists("dataset")) {
temp_dataset <-
read.csv(file, header = FALSE, stringsAsFactors = FALSE)
dataset <- rbind(dataset, temp_dataset)
rm(temp_dataset)
}
}
df <- data.frame(A = dataset$V1, B = dataset$V2)
df$D <- paste(df$A, df$B, sep = " ")
mydata.vectors <- df$D
# define "tolower error handling" function
try.tolower = function(x)
{
# create missing value
y = NA
# tryCatch error
try_error = tryCatch(
tolower(x),
error = function(e)
e
)
# if not an error
if (!inherits(try_error, "error"))
y = tolower(x)
# result
return(y)
}
#Change all variation to lower
list$Variation <- sapply(list$Variation, try.tolower)
list$lowerLanguage <- sapply(list$Language, try.tolower)
# build a corpus
mydata.corpus <- Corpus(VectorSource(mydata.vectors))
mydata.corpus <- tm_map(mydata.corpus,
content_transformer(function(x)
iconv(x, to = 'UTF-8-MAC', sub = 'byte')),
mc.cores = 1)
# remove unnecessary spaces
mydata.corpus <- tm_map(mydata.corpus, stripWhitespace)
# make each letter lowercase
mydata.corpus <-
tm_map(mydata.corpus, content_transformer(tolower), mc.cores = 1)
# replace ++ and #
replacePlus <-
content_transformer(function(x) {
return (gsub("++", "pp", x, fixed = TRUE))
})
replaceHash <-
content_transformer(function(x) {
return (gsub("#", "csharp", x, fixed = TRUE))
})
#mydata.corpus <- tm_map(mydata.corpus, replacePlus, mc.cores = 1)
#mydata.corpus <- tm_map(mydata.corpus, replaceHash, mc.cores = 1)
# remove non-alphanumeric characters
removeNonAlphaNumeric <-
content_transformer(function(x) {
return (gsub("[^a-zA-Z2-4+#]", " ", x))
})
mydata.corpus <-
tm_map(mydata.corpus, removeNonAlphaNumeric, mc.cores = 1)
# remove generic and custom stopwords
mydata.corpus <-
tm_map(mydata.corpus, removeWords, stopwords('english'))
# replace all the variations
for (c in seq(mydata.corpus))
{
for (l in 1:length(list$Language)) {
k <- strsplit(list$Variation[l], ",")
if (length(k[[1]] != 0)) {
for (v in 1:length(k[[1]])) {
mydata.corpus[[c]] <-
gsub(paste("\\b", k[[1]][v], "\\b", sep = ""),
list$lowerLanguage[l],
mydata.corpus[[c]])
}
}
}
}
# convert to Plain Text Document
mydata.corpus <- tm_map(mydata.corpus, PlainTextDocument)
# build a term-document matrix
mydata.tdm <- TermDocumentMatrix(mydata.corpus)
tdm.onlytags <-
mydata.tdm[rownames(mydata.tdm) %in% list$lowerLanguage, ]
#top10
freq <- sort(rowSums(as.matrix(tdm.onlytags)), decreasing = TRUE)
wf <- data.frame(lowerLanguage = names(freq), freq = freq)
list$industry <- wf[match(list$lowerLanguage, wf$lowerLanguage), 2]
colnames(list)[colnames(list) == 'industry'] <- industry
}
View(wf)
View(list)
View(list)
mydata.tdm$dimnames$Terms
mydata.tdm$dimnames$Terms[1:100]
library(ggplot2)
library(plyr)
library(wordcloud)
library(RColorBrewer)
library(stringr)
library(tm)
library(ggdendro)
library(dendextend)
library(ape)
library(rjson)
library(networkD3)
library(fpc)
library(cluster)
setwd("~/ParseHubData/Source_Data/tech_list")
list <-
read.csv("lang_list_lf.csv",
header = TRUE,
stringsAsFactors = TRUE)
setwd("~/ParseHubData/Source_Data/industry")
industry_list <-
c("healthcare", "retail")
for (industry in industry_list) {
setwd(paste("~/ParseHubData/Source_Data/", industry, sep = ""))
file_list <- list.files(pattern = "results.csv")
for (file in file_list) {
# if the merged dataset doesn't exist, create it
if (!exists("dataset")) {
dataset <- read.csv(file, header = FALSE, stringsAsFactors = FALSE)
}
# if the merged dataset does exist, append to it
if (exists("dataset")) {
temp_dataset <-
read.csv(file, header = FALSE, stringsAsFactors = FALSE)
dataset <- rbind(dataset, temp_dataset)
rm(temp_dataset)
}
}
df <- data.frame(A = dataset$V1, B = dataset$V2)
df$D <- paste(df$A, df$B, sep = " ")
mydata.vectors <- df$D
# define "tolower error handling" function
try.tolower = function(x)
{
# create missing value
y = NA
# tryCatch error
try_error = tryCatch(
tolower(x),
error = function(e)
e
)
# if not an error
if (!inherits(try_error, "error"))
y = tolower(x)
# result
return(y)
}
#Change all variation to lower
list$Variation <- sapply(list$Variation, try.tolower)
list$lowerLanguage <- sapply(list$Language, try.tolower)
# build a corpus
mydata.corpus <- Corpus(VectorSource(mydata.vectors))
mydata.corpus <- tm_map(mydata.corpus,
content_transformer(function(x)
iconv(x, to = 'UTF-8-MAC', sub = 'byte')),
mc.cores = 1)
# remove unnecessary spaces
mydata.corpus <- tm_map(mydata.corpus, stripWhitespace)
# make each letter lowercase
mydata.corpus <-
tm_map(mydata.corpus, content_transformer(tolower), mc.cores = 1)
# replace ++ and #
replacePlus <-
content_transformer(function(x) {
return (gsub("++", "pp", x, fixed = TRUE))
})
replaceHash <-
content_transformer(function(x) {
return (gsub("#", "csharp", x, fixed = TRUE))
})
#mydata.corpus <- tm_map(mydata.corpus, replacePlus, mc.cores = 1)
#mydata.corpus <- tm_map(mydata.corpus, replaceHash, mc.cores = 1)
# remove non-alphanumeric characters
removeNonAlphaNumeric <-
content_transformer(function(x) {
return (gsub("[^a-zA-Z2-4+#]", " ", x))
})
mydata.corpus <-
tm_map(mydata.corpus, removeNonAlphaNumeric, mc.cores = 1)
# remove generic and custom stopwords
mydata.corpus <-
tm_map(mydata.corpus, removeWords, stopwords('english'))
# replace all the variations
for (c in seq(mydata.corpus))
{
for (l in 1:length(list$Language)) {
k <- strsplit(list$Variation[l], ",")
if (length(k[[1]] != 0)) {
for (v in 1:length(k[[1]])) {
mydata.corpus[[c]] <-
gsub(paste("\\b", k[[1]][v], "\\b", sep = ""),
list$lowerLanguage[l],
mydata.corpus[[c]])
}
}
}
}
# convert to Plain Text Document
mydata.corpus <- tm_map(mydata.corpus, PlainTextDocument)
# build a term-document matrix
mydata.tdm <- TermDocumentMatrix(mydata.corpus)
tdm.onlytags <-
mydata.tdm[rownames(mydata.tdm) %in% list$lowerLanguage, ]
#top10
freq <- sort(rowSums(as.matrix(tdm.onlytags)), decreasing = TRUE)
wf <- data.frame(lowerLanguage = names(freq), freq = freq)
list$industry <- wf[match(list$lowerLanguage, wf$lowerLanguage), 2]
colnames(list)[colnames(list) == 'industry'] <- industry
}
View(list)
mydata.tdm$dimnames$Terms[1000:1500]
mydata.tdm$dimnames$Terms[100:1500]
list$Total <- rowSums(list)
list$Total <- rowSums(as.matrix(list))
list$Total <- colSums(as.matrix(list))
View(list)
list$Total <- colSums(as.matrix(list[,4:5]))
View(list)
list$Total <- rowSums(as.matrix(list[,4:5]))
View(list)
list[is.na(list)] <- 0
list$Total <- colSums(as.matrix(list[,4:5]))
View(list)
# convert to Plain Text Document
mydata.corpus <- tm_map(mydata.corpus, PlainTextDocument)
# build a term-document matrix
mydata.tdm <- TermDocumentMatrix(mydata.corpus)
tdm.onlytags <-
mydata.tdm[rownames(mydata.tdm) %in% list$lowerLanguage, ]
#top10
freq <- sort(rowSums(as.matrix(tdm.onlytags)), decreasing = TRUE)
wf <- data.frame(lowerLanguage = names(freq), freq = freq)
list$industry <- wf[match(list$lowerLanguage, wf$lowerLanguage), 2]
colnames(list)[colnames(list) == 'industry'] <- industry
list[is.na(list)] <- 0
list$Total <- colSums(as.matrix(list[,4:5]))
View(list)
list$Total <- colSums(as.matrix(list[,c(list$healthcare, list$retail)]))
list$NewTotal <- list$healthcare + list$retail
View(list)
library(ggplot2)
library(plyr)
library(wordcloud)
library(RColorBrewer)
library(stringr)
library(tm)
library(ggdendro)
library(dendextend)
library(ape)
library(rjson)
library(networkD3)
library(fpc)
library(cluster)
setwd("~/ParseHubData/Source_Data/tech_list")
list <-
read.csv("lang_list_lf.csv",
header = TRUE,
stringsAsFactors = TRUE)
setwd("~/ParseHubData/Source_Data/industry")
industry_list <-
c("healthcare", "retail", "telecom", "tech", "financial")
for (industry in industry_list) {
setwd(paste("~/ParseHubData/Source_Data/", industry, sep = ""))
file_list <- list.files(pattern = "results.csv")
for (file in file_list) {
# if the merged dataset doesn't exist, create it
if (!exists("dataset")) {
dataset <- read.csv(file, header = FALSE, stringsAsFactors = FALSE)
}
# if the merged dataset does exist, append to it
if (exists("dataset")) {
temp_dataset <-
read.csv(file, header = FALSE, stringsAsFactors = FALSE)
dataset <- rbind(dataset, temp_dataset)
rm(temp_dataset)
}
}
df <- data.frame(A = dataset$V1, B = dataset$V2)
df$D <- paste(df$A, df$B, sep = " ")
mydata.vectors <- df$D
# define "tolower error handling" function
try.tolower = function(x)
{
# create missing value
y = NA
# tryCatch error
try_error = tryCatch(
tolower(x),
error = function(e)
e
)
# if not an error
if (!inherits(try_error, "error"))
y = tolower(x)
# result
return(y)
}
#Change all variation to lower
list$Variation <- sapply(list$Variation, try.tolower)
list$lowerLanguage <- sapply(list$Language, try.tolower)
# build a corpus
mydata.corpus <- Corpus(VectorSource(mydata.vectors))
mydata.corpus <- tm_map(mydata.corpus,
content_transformer(function(x)
iconv(x, to = 'UTF-8-MAC', sub = 'byte')),
mc.cores = 1)
# remove unnecessary spaces
mydata.corpus <- tm_map(mydata.corpus, stripWhitespace)
# make each letter lowercase
mydata.corpus <-
tm_map(mydata.corpus, content_transformer(tolower), mc.cores = 1)
# replace ++ and #
replacePlus <-
content_transformer(function(x) {
return (gsub("++", "pp", x, fixed = TRUE))
})
replaceHash <-
content_transformer(function(x) {
return (gsub("#", "csharp", x, fixed = TRUE))
})
#mydata.corpus <- tm_map(mydata.corpus, replacePlus, mc.cores = 1)
#mydata.corpus <- tm_map(mydata.corpus, replaceHash, mc.cores = 1)
# remove non-alphanumeric characters
removeNonAlphaNumeric <-
content_transformer(function(x) {
return (gsub("[^a-zA-Z2-4+#]", " ", x))
})
mydata.corpus <-
tm_map(mydata.corpus, removeNonAlphaNumeric, mc.cores = 1)
# remove generic and custom stopwords
mydata.corpus <-
tm_map(mydata.corpus, removeWords, stopwords('english'))
# replace all the variations
for (c in seq(mydata.corpus))
{
for (l in 1:length(list$Language)) {
k <- strsplit(list$Variation[l], ",")
if (length(k[[1]] != 0)) {
for (v in 1:length(k[[1]])) {
mydata.corpus[[c]] <-
gsub(paste("\\b", k[[1]][v], "\\b", sep = ""),
list$lowerLanguage[l],
mydata.corpus[[c]])
}
}
}
}
# convert to Plain Text Document
mydata.corpus <- tm_map(mydata.corpus, PlainTextDocument)
# build a term-document matrix
mydata.tdm <- TermDocumentMatrix(mydata.corpus)
tdm.onlytags <-
mydata.tdm[rownames(mydata.tdm) %in% list$lowerLanguage, ]
#top10
freq <- sort(rowSums(as.matrix(tdm.onlytags)), decreasing = TRUE)
wf <- data.frame(lowerLanguage = names(freq), freq = freq)
list$industry <- wf[match(list$lowerLanguage, wf$lowerLanguage), 2]
colnames(list)[colnames(list) == 'industry'] <- industry
}
list[is.na(list)] <- 0
list$NewTotal <- list$healthcare + list$retail + list$financial + list$telecom + list$tech
View(list)
